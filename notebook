# ===========================
# ğŸ“¦ 1. Import Dependencies
# ===========================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set consistent style and colors
sns.set_style("whitegrid")
custom_palette = ["#ADD8E6", "#FFA500", "#FFFFFF"]
sns.set_palette(custom_palette)

# ================================
# ğŸ“¥ 2. Load and Preview the Data
# ================================
print("Loading dataset...")
url = "https://raw.githubusercontent.com/tommywood81/GCN_Data/refs/heads/data/Fraudulent_E-Commerce_Transaction_Data_2.csv"
df = pd.read_csv(url)

print("\nâœ… Dataset Loaded!")
print("Shape of the dataset:", df.shape)
print("\nFirst few rows:\n")
print(df.head())

print("\nğŸ“‹ Data Types:\n")
print(df.dtypes)

print("\nğŸ” Missing Values:\n")
print(df.isnull().sum())

# ================================
# ğŸ“† 3. Date Formatting
# ================================
print("\nConverting 'Transaction Date' to datetime format...")
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'], errors='coerce')

print("Creating 'Account Creation Date' by subtracting account age from transaction date...")
df['Account Creation Date'] = df['Transaction Date'] - pd.to_timedelta(df['Account Age Days'], unit='D')

# ======================================
# ğŸš© 4. Outlier Detection (Customer Age)
# ======================================
print("\nDetecting outliers in 'Customer Age' using IQR...")

def find_age_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return series[(series < lower_bound) | (series > upper_bound)]

age_outliers = find_age_outliers(df['Customer Age'])
print(f"ğŸ”º Outliers in Customer Age: {len(age_outliers)} found")
print("Example outlier ages:", age_outliers.unique())

# Custom logic for age group breakdown
under_18 = df[df['Customer Age'] < 18].shape[0]
under_0 = df[df['Customer Age'] < 0].shape[0]
over_18 = df[df['Customer Age'] >= 18].shape[0]

print(f"\nğŸ“Š Customer Age Breakdown:")
print(f"- Under 18: {under_18}")
print(f"- Less than 0 (invalid): {under_0}")
print(f"- 18 and over: {over_18}")

# ğŸ” Count how many under 18 are fraudulent
under_18_fraud_count = df[(df['Customer Age'] < 18) & (df['Is Fraudulent'] == 1)].shape[0]
print(f"- Of those under 18, {under_18_fraud_count} were marked as fraudulent.")

# ====================================
# ğŸ“Š 5. Data Distribution Visualizations
# ====================================

# Fraudulent vs Legitimate Transactions
plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='Is Fraudulent')
plt.title("ğŸ•µï¸ Fraudulent vs Legitimate Transactions")
plt.xlabel("Is Fraudulent")
plt.ylabel("Transaction Count")
plt.tight_layout()
plt.show()

# Transaction Amount Histogram
plt.figure(figsize=(8, 5))
sns.histplot(df['Transaction Amount'], bins=50, kde=True, color=custom_palette[0])
plt.title("ğŸ’° Transaction Amount Distribution")
plt.xlabel("Amount")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()
print("Note: High-value transactions are rare, but not necessarily outliers. We'll keep them.")

# Customer Age Distribution
plt.figure(figsize=(8, 5))
sns.histplot(df['Customer Age'], bins=30, kde=True, color=custom_palette[1])
plt.title("ğŸ‘¤ Customer Age Distribution")
plt.xlabel("Age")
plt.tight_layout()
plt.show()

# Fraud by Hour of Day
plt.figure(figsize=(10, 5))
sns.countplot(data=df, x='Transaction Hour', hue='Is Fraudulent')
plt.title("â° Fraud by Hour of Transaction")
plt.xlabel("Hour of Day")
plt.ylabel("Transaction Count")
plt.tight_layout()
plt.show()

# Payment Method vs Fraud
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='Payment Method', hue='Is Fraudulent')
plt.title("ğŸ’³ Fraud Distribution by Payment Method")
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

# Device Type vs Fraud
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='Device Used', hue='Is Fraudulent')
plt.title("ğŸ“± Fraud by Device Type")
plt.tight_layout()
plt.show()

# Top 10 Customer Locations
top_locations = df['Customer Location'].value_counts().head(10)
plt.figure(figsize=(10, 5))
sns.barplot(x=top_locations.index, y=top_locations.values)
plt.title("ğŸŒ Top 10 Customer Locations by Volume")
plt.ylabel("Transaction Count")
plt.xlabel("Location")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# =====================================
# ğŸ”— 6. Correlation Analysis (Numeric)
# =====================================
corr_cols = ['Transaction Amount', 'Quantity', 'Customer Age', 'Account Age Days', 'Transaction Hour', 'Is Fraudulent']
corr = df[corr_cols].corr()

plt.figure(figsize=(10, 6))
sns.heatmap(corr, annot=True, fmt=".2f", cmap=sns.color_palette(["#ADD8E6", "#FFFFFF", "#FFA500"], as_cmap=True))
plt.title("ğŸ“ˆ Correlation Matrix (Key Numerical Features)")
plt.tight_layout()
plt.show()

# =====================================
# ğŸ§¾ 7. Summary Observations
# =====================================
print("\nğŸ“ Key Observations:")

most_common_location = df['Customer Location'].mode()[0]
most_fraud_hour = df[df['Is Fraudulent'] == 1]['Transaction Hour'].mode()[0]
most_fraud_device = df[df['Is Fraudulent'] == 1]['Device Used'].mode()[0]

print(f"- Dataset includes {len(df)} transactions.")
print(f"- Most common customer location: {most_common_location}")
print(f"- Most frequent fraud occurs around hour: {most_fraud_hour}")
print(f"- The device type most commonly associated with fraud is: {most_fraud_device}")
print(f"- There are {len(age_outliers)} likely outliers in customer age (e.g., -2 years old).")
print(f"- There are {under_0} customers with impossible negative ages â€” possible data errors.")

# =====================================
# âš–ï¸ 8. Undersampling for Class Balance
# =====================================
print("\nâš–ï¸ Performing undersampling to create 1:3 fraud-to-legit ratio...")

# Separate fraud and legit
fraud_df = df[df['Is Fraudulent'] == 1]
legit_df = df[df['Is Fraudulent'] == 0]

# Undersample legit transactions to 3x fraud count
target_legit_count = 3 * len(fraud_df)
legit_sample = legit_df.sample(n=target_legit_count, random_state=42)

# Combine into new balanced dataframe
balanced_df = pd.concat([fraud_df, legit_sample]).sample(frac=1, random_state=42).reset_index(drop=True)
print(f"âœ… Balanced dataset: {balanced_df.shape[0]} rows ({len(fraud_df)} fraud, {len(legit_sample)} legit)")

# ================================================
# ğŸ“ˆ 9. Correlation Heatmap After Undersampling
# ================================================
print("\nğŸ” Generating correlation heatmap on balanced dataset...")

corr_cols = ['Transaction Amount', 'Quantity', 'Customer Age', 'Account Age Days', 'Transaction Hour', 'Is Fraudulent']
balanced_corr = balanced_df[corr_cols].corr()

plt.figure(figsize=(10, 6))
sns.heatmap(balanced_corr, annot=True, fmt=".2f", cmap=sns.color_palette(["#ADD8E6", "#FFFFFF", "#FFA500"], as_cmap=True))
plt.title("ğŸ“ˆ Correlation Matrix After Undersampling (Fraud vs Features)")
plt.tight_layout()
plt.show()

# ================================================
# ğŸ“„ 10. Print Heatmap Correlation Matrix as Text
# ================================================
print("\nğŸ§¾ Correlation Matrix (Text Format):")
print(balanced_corr.round(3).to_string())

"""Key Correlation Insights (with Is Fraudulent)
Feature
Correlation
Interpretation
Transaction Amount
+0.320
âœ… Moderate positive correlation. Fraudulent transactions tend to involve higher amounts. This is a useful signal for detection.
Quantity
â€“0.010
âŒ Very weak correlation. Number of items purchased does not significantly differ between fraud and legit. Likely not useful.
Customer Age
+0.006
âŒ Negligible correlation. Age has almost no direct link to fraud likelihood. Might need to explore non-linear or interaction effects.
Account Age Days
â€“0.272
âœ… Moderate negative correlation. Fraud is more likely when accounts are newer (i.e., low account age). Very useful feature.
Transaction Hour
â€“0.238
âœ… Weak to moderate negative correlation. Fraud tends to happen at certain times of day, possibly off-hours. This is insightful.

Summary of What This Means
	â€¢	Most Predictive Features (based on linear correlation):
	â€¢	Transaction Amount â†’ higher value = more likely to be fraud
	â€¢	Account Age Days â†’ newer account = more fraud
	â€¢	Transaction Hour â†’ fraud tends to occur during specific times (e.g., night or early morning)
	â€¢	Least Useful Features:
	â€¢	Quantity and Customer Age show almost no correlation. You may consider dropping them unless they interact with other variables in non-linear models
  (like trees or GNNs).

""" prototype of data_cleaning

# ===============================
# ğŸ“‚ data_cleaning.py
# ===============================

import pandas as pd
from abc import ABC, abstractmethod

# ===============================
# ğŸ§¼ Strategy Base Class
# ===============================
class DataCleaningStrategy(ABC):
    """
    Abstract base class for data cleaning strategies.

    All cleaning strategies must implement the `clean()` method.
    """
    @abstractmethod
    def clean(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Cleans the provided DataFrame according to a specific strategy.

        Parameters:
            df (pd.DataFrame): The raw transaction dataset.

        Returns:
            pd.DataFrame: The cleaned DataFrame.
        """
        pass

# ======================================
# ğŸ§½ Default Cleaning Strategy
# ======================================
class DefaultFraudCleaningStrategy(DataCleaningStrategy):
    """
    Default cleaning strategy for fraud detection data.

    This strategy:
    - Removes customers with age < 18 (underage or fake profiles).
    - Removes rows with negative account age (data entry errors).
    - Removes rows with non-positive transaction amounts.
    - Removes rows with missing values in critical columns.
    """

    def clean(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Cleans the fraud dataset using default fraud-focused rules.

        Cleaning Rules:
        - Drops rows where 'Customer Age' is less than 18.
        - Drops rows where 'Account Age Days' is negative.
        - Drops rows where 'Transaction Amount' is zero or negative.
        - Drops rows with missing values in key columns:
            ['Transaction Amount', 'Customer Age', 'Account Age Days', 'Transaction Hour', 'Is Fraudulent']

        Parameters:
            df (pd.DataFrame): The raw transaction data.

        Returns:
            pd.DataFrame: A cleaned version of the DataFrame with invalid entries removed.
        """
        original_shape = df.shape
        print(f"\nğŸ“Š Starting cleaning: {original_shape[0]} rows, {original_shape[1]} columns")

        # Drop customers under 18
        df = df[df['Customer Age'] >= 18]
        print(f"âœ… Dropped customers under 18. Remaining: {df.shape[0]} rows")

        # Drop negative account age values
        df = df[df['Account Age Days'] >= 0]
        print(f"âœ… Dropped rows with negative account age. Remaining: {df.shape[0]} rows")

        # Drop zero or negative transaction amounts
        df = df[df['Transaction Amount'] > 0]
        print(f"âœ… Dropped zero or negative transaction amounts. Remaining: {df.shape[0]} rows")

        # Drop rows with missing values in key columns
        required_cols = [
            'Transaction Amount', 'Customer Age',
            'Account Age Days', 'Transaction Hour', 'Is Fraudulent'
        ]
        before_dropna = df.shape[0]
        df = df.dropna(subset=required_cols)
        print(f"âœ… Dropped rows with missing values in required fields ({before_dropna - df.shape[0]} rows removed)")

        print(f"\nğŸ§¼ Cleaning complete. Final shape: {df.shape[0]} rows, {df.shape[1]} columns")
        return df

# ===============================
# ğŸ§° Cleaning Context Class
# ===============================
class DataCleaner:
    """
    Context class for applying a data cleaning strategy.

    Allows the cleaning behavior to be swapped by injecting a different strategy.
    """

    def __init__(self, strategy: DataCleaningStrategy):
        """
        Initializes the DataCleaner with a specific strategy.

        Parameters:
            strategy (DataCleaningStrategy): The strategy to use for cleaning.
        """
        self.strategy = strategy

    def clean(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Applies the configured cleaning strategy to the input DataFrame.

        Parameters:
            df (pd.DataFrame): The raw input data.

        Returns:
            pd.DataFrame: Cleaned data as per the active strategy.
        """
        return self.strategy.clean(df)

        """


